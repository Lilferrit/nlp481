{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Dict, List\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"1.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmptyFrameDict(\n",
    "    input_key: str,\n",
    "    output_key: str,\n",
    "    dataset_dict: DatasetDict\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    dataframe_dict = dict()\n",
    "\n",
    "    for curr_name, curr_dataset in dataset_dict.items():\n",
    "        next_dataframe = pd.DataFrame.from_dict({\n",
    "            input_key: curr_dataset[input_key],\n",
    "            output_key: [\"\"] * len(curr_dataset[input_key])\n",
    "        })\n",
    "\n",
    "        dataframe_dict[curr_name] = next_dataframe\n",
    "\n",
    "    return dataframe_dict\n",
    "\n",
    "def cacheFrame(\n",
    "    data_frame: pd.DataFrame,\n",
    "    cache_dir: Path,\n",
    "    cache_entry_name: str\n",
    ") -> None:\n",
    "    file_name = f\"{cache_entry_name}.cache.parquet\"\n",
    "    data_frame.to_parquet(\n",
    "        os.path.join(\n",
    "            cache_dir,\n",
    "            file_name\n",
    "        ),\n",
    "        engine = \"pyarrow\"\n",
    "    )\n",
    "\n",
    "def cacheFrameDict(\n",
    "    cache_dir: Path,\n",
    "    dataframe_dict: Dict[str, pd.DataFrame],\n",
    "    prefix_name: str = None,\n",
    ") -> None:\n",
    "    for curr_name, curr_dataframe in dataframe_dict.items():\n",
    "        file_name = curr_name\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        cacheFrame(\n",
    "            curr_dataframe,\n",
    "            cache_dir,\n",
    "            file_name\n",
    "        )\n",
    "\n",
    "def loadFrameDict(\n",
    "    cache_dir: Path,\n",
    "    dataframe_names: List[str],\n",
    "    prefix_name: str = None,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    dataframe_dict = dict()\n",
    "\n",
    "    for curr_name in dataframe_names:\n",
    "        file_name = f\"{curr_name}.cache.parquet\"\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        dataframe_dict[curr_name] = pd.read_parquet(\n",
    "            os.path.join(\n",
    "                cache_dir,\n",
    "                file_name\n",
    "            ),\n",
    "            engine = \"pyarrow\"\n",
    "        )\n",
    "\n",
    "    return dataframe_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cells if Creating Fresh Frame Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_frame_dict = getEmptyFrameDict(\"article\", \"t5_large_output\", cnn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheFrameDict(\"./cache\", cnn_frame_dict, \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUn Cell if Using Cached Frame Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_cnn_frame_dict = loadFrameDict(\"./cache\", cnn_dataset.keys(), \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferDataFrameDict(\n",
    "    dataframe_dict: Dict[str, pd.DataFrame],\n",
    "    batch_size: int,\n",
    "    input_key: str = \"article\",\n",
    "    output_key: str = \"t5_large_output\",\n",
    "    prefix: str = \"summarize: \",\n",
    "    max_input_length: int = 1024,\n",
    "    max_output_length: int = 1024,\n",
    "    cache_location: str = None,\n",
    "    dataset_name: str = None,\n",
    "    batches_per_cache_write: int = None\n",
    ") -> None:\n",
    "    using_cache = all(x is not None for x in [cache_location, dataset_name, batches_per_cache_write])\n",
    "    print(using_cache)\n",
    "\n",
    "    for curr_name, curr_dataframe in dataframe_dict.items():\n",
    "        chunks_iter = np.array_split(curr_dataframe, (len(curr_dataframe) // batch_size) + 1)\n",
    "        row_counter = 0\n",
    "        \n",
    "        for chunk_idx, curr_chunk in enumerate(tqdm(chunks_iter)):\n",
    "            curr_chunk_inputs = list(curr_chunk[input_key])\n",
    "            curr_chunk_outputs = list(curr_chunk[output_key])\n",
    "            is_cached = all(x != \"\" for x in curr_chunk_outputs)\n",
    "\n",
    "            if is_cached:\n",
    "                row_counter += len(curr_chunk_outputs)\n",
    "                continue\n",
    "\n",
    "            inputs = [prefix + doc for doc in curr_chunk_inputs]\n",
    "            input_ids = tokenizer(\n",
    "                inputs, \n",
    "                return_tensors = \"pt\",\n",
    "                max_length = max_input_length,\n",
    "                truncation = True,\n",
    "                padding = True,\n",
    "            ).input_ids.to(\"cuda:0\")\n",
    "\n",
    "            outputs = model.generate(input_ids, max_new_tokens = max_output_length)\n",
    "            outputs.to(\"cpu\")\n",
    "            decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "\n",
    "            out_column_index = curr_dataframe.columns.get_loc(output_key)\n",
    "            end_row_index = row_counter + len(decoded_output)\n",
    "            curr_dataframe.iloc[row_counter : end_row_index, out_column_index] = decoded_output\n",
    "            row_counter += len(curr_chunk_outputs)\n",
    "\n",
    "            if ((chunk_idx + 1) % batches_per_cache_write) == 0:\n",
    "                cacheFrame(\n",
    "                    curr_dataframe,\n",
    "                    cache_location,\n",
    "                    f\"{dataset_name}_{curr_name}\"\n",
    "                )\n",
    "            \n",
    "        cacheFrame(\n",
    "            curr_dataframe,\n",
    "            cache_location,\n",
    "            f\"{dataset_name}_{curr_name}\"\n",
    "        )\n",
    "\n",
    "inferDataFrameDict(\n",
    "    cnn_frame_dict,\n",
    "    8,\n",
    "    cache_location = \"./cache\",\n",
    "    dataset_name = \"cnn_dm_distill\",\n",
    "    batches_per_cache_write = 2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_frame_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face doesn't provide an easy way to load a DatasetDict from Pandas,\n",
    "# so loadDatasetFromCachedDataframe pulls the latest cache entry instead\n",
    "def loadDatasetFromCachedDataframe(\n",
    "    cache_dir: Path,\n",
    "    dataframe_names: List[str],\n",
    "    prefix_name: str = None,\n",
    ") -> DatasetDict:\n",
    "    file_dict = dict()\n",
    "\n",
    "    for curr_name in dataframe_names:\n",
    "        file_name = f\"{curr_name}.cache.parquet\"\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        file_dict[curr_name] = os.path.join(cache_dir, file_name)\n",
    "\n",
    "    return DatasetDict.from_parquet(file_dict)\n",
    "\n",
    "cnn_distill_dataset = loadDatasetFromCachedDataframe(\"./cache\", cnn_dataset.keys(), \"cnn_dm_distill\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_swag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
