{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dubcar/anaconda3/envs/yolo_swag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Dict, List\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"1.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmptyFrameDict(\n",
    "    input_key: str,\n",
    "    output_key: str,\n",
    "    dataset_dict: DatasetDict\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    dataframe_dict = dict()\n",
    "\n",
    "    for curr_name, curr_dataset in dataset_dict.items():\n",
    "        next_dataframe = pd.DataFrame.from_dict({\n",
    "            input_key: curr_dataset[input_key],\n",
    "            output_key: [\"\"] * len(curr_dataset[input_key])\n",
    "        })\n",
    "\n",
    "        dataframe_dict[curr_name] = next_dataframe\n",
    "\n",
    "    return dataframe_dict\n",
    "\n",
    "def cacheFrame(\n",
    "    data_frame: pd.DataFrame,\n",
    "    cache_dir: Path,\n",
    "    cache_entry_name: str\n",
    ") -> None:\n",
    "    file_name = f\"{cache_entry_name}.cache.parquet\"\n",
    "    data_frame.to_parquet(\n",
    "        os.path.join(\n",
    "            cache_dir,\n",
    "            file_name\n",
    "        ),\n",
    "        engine = \"pyarrow\"\n",
    "    )\n",
    "\n",
    "def cacheFrameDict(\n",
    "    cache_dir: Path,\n",
    "    dataframe_dict: Dict[str, pd.DataFrame],\n",
    "    prefix_name: str = None,\n",
    ") -> None:\n",
    "    for curr_name, curr_dataframe in dataframe_dict.items():\n",
    "        file_name = curr_name\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        cacheFrame(\n",
    "            curr_dataframe,\n",
    "            cache_dir,\n",
    "            file_name\n",
    "        )\n",
    "\n",
    "def loadFrameDict(\n",
    "    cache_dir: Path,\n",
    "    dataframe_names: List[str],\n",
    "    prefix_name: str = None,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    dataframe_dict = dict()\n",
    "\n",
    "    for curr_name in dataframe_names:\n",
    "        file_name = f\"{curr_name}.cache.parquet\"\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        dataframe_dict[curr_name] = pd.read_parquet(\n",
    "            os.path.join(\n",
    "                cache_dir,\n",
    "                file_name\n",
    "            ),\n",
    "            engine = \"pyarrow\"\n",
    "        )\n",
    "\n",
    "    return dataframe_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cells if Creating Fresh Frame Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_frame_dict = getEmptyFrameDict(\"article\", \"t5_large_output\", cnn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheFrameDict(\"./cache\", cnn_frame_dict, \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUn Cell if Using Cached Frame Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_cnn_frame_dict = loadFrameDict(\"./cache\", cnn_dataset.keys(), \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dubcar/anaconda3/envs/yolo_swag/lib/python3.9/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "  0%|          | 0/35890 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE HIT LESS GO\n",
      "CACHE HIT LESS GO\n",
      "CACHE HIT LESS GO\n",
      "CACHE HIT LESS GO\n",
      "CACHE HIT LESS GO\n",
      "CACHE HIT LESS GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/35890 [00:03<6:26:30,  1.55it/s]\n",
      "  0%|          | 5/1672 [00:22<2:02:41,  4.42s/it]\n",
      "  0%|          | 5/1437 [00:23<1:54:27,  4.80s/it]\n"
     ]
    }
   ],
   "source": [
    "def inferDataFrameDict(\n",
    "    dataframe_dict: Dict[str, pd.DataFrame],\n",
    "    batch_size: int,\n",
    "    input_key: str = \"article\",\n",
    "    output_key: str = \"t5_large_output\",\n",
    "    prefix: str = \"summarize: \",\n",
    "    max_input_length: int = 1024,\n",
    "    max_output_length: int = 1024,\n",
    "    cache_location: str = None,\n",
    "    dataset_name: str = None,\n",
    "    batches_per_cache_write: int = None\n",
    ") -> None:\n",
    "    using_cache = all(x is not None for x in [cache_location, dataset_name, batches_per_cache_write])\n",
    "    print(using_cache)\n",
    "\n",
    "    for curr_name, curr_dataframe in dataframe_dict.items():\n",
    "        chunks_iter = np.array_split(curr_dataframe, (len(curr_dataframe) // batch_size) + 1)\n",
    "        row_counter = 0\n",
    "        \n",
    "        for chunk_idx, curr_chunk in enumerate(tqdm(chunks_iter)):\n",
    "            curr_chunk_inputs = list(curr_chunk[input_key])\n",
    "            curr_chunk_outputs = list(curr_chunk[output_key])\n",
    "            is_cached = all(x != \"\" for x in curr_chunk_outputs)\n",
    "\n",
    "            if is_cached:\n",
    "                row_counter += len(curr_chunk_outputs)\n",
    "                continue\n",
    "\n",
    "            inputs = [prefix + doc for doc in curr_chunk_inputs]\n",
    "            input_ids = tokenizer(\n",
    "                inputs, \n",
    "                return_tensors = \"pt\",\n",
    "                max_length = max_input_length,\n",
    "                truncation = True,\n",
    "                padding = True,\n",
    "            ).input_ids.to(\"cuda:0\")\n",
    "\n",
    "            outputs = model.generate(input_ids, max_new_tokens = max_output_length)\n",
    "            outputs.to(\"cpu\")\n",
    "            decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "\n",
    "            out_column_index = curr_dataframe.columns.get_loc(output_key)\n",
    "            end_row_index = row_counter + len(decoded_output)\n",
    "            curr_dataframe.iloc[row_counter : end_row_index, out_column_index] = decoded_output\n",
    "            row_counter += len(curr_chunk_outputs)\n",
    "\n",
    "            if ((chunk_idx + 1) % batches_per_cache_write) == 0:\n",
    "                cacheFrame(\n",
    "                    curr_dataframe,\n",
    "                    cache_location,\n",
    "                    f\"{dataset_name}_{curr_name}\"\n",
    "                )\n",
    "            \n",
    "        cacheFrame(\n",
    "            curr_dataframe,\n",
    "            cache_location,\n",
    "            f\"{dataset_name}_{curr_name}\"\n",
    "        )\n",
    "\n",
    "inferDataFrameDict(\n",
    "    cnn_frame_dict,\n",
    "    8,\n",
    "    cache_location = \"./cache\",\n",
    "    dataset_name = \"cnn_dm_distill\",\n",
    "    batches_per_cache_write = 2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>t5_large_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN)The Palestinian Authority officially beca...</td>\n",
       "      <td>the ICC opens a preliminary examination into t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)Never mind cats having nine lives. A stra...</td>\n",
       "      <td>a stray dog in Washington state has used up at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN)If you've been following the news lately,...</td>\n",
       "      <td>\"long live Zarif,\" crowds chanted as he arrive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN)Five Americans who were monitored for thr...</td>\n",
       "      <td>five americans who were exposed to Ebola in we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)A Duke student has admitted to hanging a ...</td>\n",
       "      <td>the student will face student conduct review. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11485</th>\n",
       "      <td>Telecom watchdogs are to stop a rip-off that a...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11486</th>\n",
       "      <td>The chilling reenactment of how executions are...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>It is a week which has seen him in deep water ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11488</th>\n",
       "      <td>Despite the hype surrounding its first watch, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11489</th>\n",
       "      <td>Angus Hawley's brother has spoken of his shock...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "0      (CNN)The Palestinian Authority officially beca...   \n",
       "1      (CNN)Never mind cats having nine lives. A stra...   \n",
       "2      (CNN)If you've been following the news lately,...   \n",
       "3      (CNN)Five Americans who were monitored for thr...   \n",
       "4      (CNN)A Duke student has admitted to hanging a ...   \n",
       "...                                                  ...   \n",
       "11485  Telecom watchdogs are to stop a rip-off that a...   \n",
       "11486  The chilling reenactment of how executions are...   \n",
       "11487  It is a week which has seen him in deep water ...   \n",
       "11488  Despite the hype surrounding its first watch, ...   \n",
       "11489  Angus Hawley's brother has spoken of his shock...   \n",
       "\n",
       "                                         t5_large_output  \n",
       "0      the ICC opens a preliminary examination into t...  \n",
       "1      a stray dog in Washington state has used up at...  \n",
       "2      \"long live Zarif,\" crowds chanted as he arrive...  \n",
       "3      five americans who were exposed to Ebola in we...  \n",
       "4      the student will face student conduct review. ...  \n",
       "...                                                  ...  \n",
       "11485                                                     \n",
       "11486                                                     \n",
       "11487                                                     \n",
       "11488                                                     \n",
       "11489                                                     \n",
       "\n",
       "[11490 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_frame_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 287113 examples [00:02, 121671.85 examples/s]\n",
      "Generating validation split: 13368 examples [00:00, 126765.06 examples/s]\n",
      "Generating test split: 11490 examples [00:00, 122538.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face doesn't provide an easy way to load a DatasetDict from Pandas,\n",
    "# so loadDatasetFromCachedDataframe pulls the latest cache entry instead\n",
    "def loadDatasetFromCachedDataframe(\n",
    "    cache_dir: Path,\n",
    "    dataframe_names: List[str],\n",
    "    prefix_name: str = None,\n",
    ") -> DatasetDict:\n",
    "    file_dict = dict()\n",
    "\n",
    "    for curr_name in dataframe_names:\n",
    "        file_name = f\"{curr_name}.cache.parquet\"\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        file_dict[curr_name] = os.path.join(cache_dir, file_name)\n",
    "\n",
    "    return DatasetDict.from_parquet(file_dict)\n",
    "\n",
    "cnn_distill_dataset = loadDatasetFromCachedDataframe(\"./cache\", cnn_dataset.keys(), \"cnn_dm_distill\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_swag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
